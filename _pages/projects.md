---
layout: single
title: "Projects"
permalink: /projects/
---

## Current Projects

<details>
  <summary>
    <strong>InterVisions</strong> – Participatory AI for Intersectional Bias Auditing (2025–2026)  
    <br>
    <strong>Funding:</strong> EU – CERV &nbsp; | &nbsp; 
    <strong>Grant ID:</strong> 101214711 &nbsp; | &nbsp; 
    <strong>Budget:</strong> €245,417.34  
    <br><em style="color: #007acc; cursor: pointer;">More info</em>
  </summary>
  <br>

  <strong>Coordinated by:</strong> ALIA – Associació Cultural de Dones per a la Recerca i l’Acció <br>
  <strong>Participants:</strong><br>
  – <strong>Centre de Visió per Computador (CVC-UAB)</strong>, Research Organisation<br>
  – <strong>Diputació de Barcelona</strong>, Associated Partner<br><br>

  <strong>Goal:</strong><br>
  InterVisions aims to build a <strong>participatory bias audit tool</strong> for vision and language foundation models.  
  It integrates intersectional feminist theory, deep learning, and participatory AI practices to identify and mitigate social biases in large-scale multimodal AI systems.<br><br>

  <strong>Activities:</strong><br>
  – Community-driven workshops to audit foundation models<br>
  – Co-creation of a <strong>technical fairness benchmark</strong><br>
  – Development of <strong>intersectional impact assessment guidelines</strong><br>
  – Promotion of <strong>ethical AI practices</strong> in line with the EU Charter of Fundamental Rights<br><br>

  <strong>Keywords:</strong> Bias in AI, Ethical AI, Participatory AI, Intersectionality, Fairness Benchmark, Vision & Language Models<br><br>

  <strong>Project Website:</strong>  TBD

</details>

<hr style="border: 0; border-top: 2px dotted #c9c9c9; margin: 1.25rem 0;">

<details>
  <summary>
    <strong>FairCLIP</strong> – Training a Fair CLIP Model with Hybrid Real and Synthetic Data (2024–2025)<br>
    <strong>Funding:</strong> EuroHPC AI & Data-Intensive Applications Access Call &nbsp; | &nbsp;
    <strong>Grant ID:</strong> EHPC-AI-2024A02-040 &nbsp; | &nbsp;
    <strong>Resources:</strong> 32,000 node hours on MareNostrum5<br>
    <em style="color: #007acc; cursor: pointer;">More info</em>
  </summary>
  <br>

  <strong>Coordinated by:</strong> Universitat Autònoma de Barcelona / Computer Vision Center (Spain)<br>
  <strong>Team:</strong><br>
  – Dr. Lluis Gomez (PI) • Dr. Lei Kang • Dr. Mohamed Ali Souibgui • Mr. Francesc Net • Mr. Joan Masoliver • Dr. Sonia Ruiz • Prof. Yuki M. Asano (University of Amsterdam)<br><br>

  <strong>Objective:</strong><br>
  The <strong>FairCLIP</strong> project aims to mitigate bias in large-scale vision-language models by training a new CLIP model on a hybrid dataset combining real and synthetic data, ensuring balanced demographic representation. The project contributes to fairness in AI with both technical and ethical innovations.<br><br>

  <strong>Key Methods:</strong><br>
  – Synthetic data generation via state-of-the-art diffusion models<br>
  – Real data from the CommonPool dataset<br>
  – OpenCLIP framework for scalable training<br>
  – Contrastive learning with demographic control<br><br>

  <strong>Milestones:</strong><br>
  – Small-scale (12M samples), medium-scale (128M), and large-scale (400M) experiments<br>
  – Total: 32,000 node hours over 12 months (Aug 2024–Jul 2025)<br><br>

  <strong>Expected Outcomes:</strong><br>
  – A fairness-optimized CLIP model<br>
  – A reusable hybrid dataset<br>
  – Open-source technical deliverables<br><br>

  <strong>Keywords:</strong> Fair AI, CLIP, Synthetic Data, Bias Mitigation, Diffusion Models, Vision-Language Models, HPC<br><br>

  <strong>Code:</strong>  
  <a href="https://github.com/lluisgomez/FairCLIP" target="_blank">FairCLIP GitHub Repository</a>  
</details>

<hr style="border: 0; border-top: 2px dotted #c9c9c9; margin: 1.25rem 0;">

<details>
  <summary>
    <strong>COELI-IA</strong> – From Text to Media: A Paradigm Shift in Cultural Heritage Management (2023–2025)<br>
    <strong>Funding:</strong> INNOTEC R+D Grant (Catalonia) &nbsp; | &nbsp;
    <strong>Grant ID:</strong> RDECR20/EMT/1791/2021 &nbsp; | &nbsp;
    <strong>Budget:</strong> €195,530.02<br>
    <em style="color: #007acc; cursor: pointer;">More info</em>
  </summary>
  <br>

  <strong>Coordinated by:</strong> Nubilum SL (SME) <br>
  <strong>Research Partner:</strong> Centre de Visió per Computador (CVC), Universitat Autònoma de Barcelona<br><br>

  <strong>Objective:</strong><br>
  COELI-IA aims to revolutionize the management and dissemination of cultural heritage content by leveraging AI techniques. The project explores automatic classification, indexing, and enhanced accessibility for digital archives through multimodal models that can understand and connect text and media data.<br><br>

  <strong>Key Innovations:</strong><br>
  – Development of AI-driven cultural heritage content engines<br>
  – New interfaces and recommendation systems based on content relevance<br>
  – Fine-tuning of AI models for domain-specific archives<br><br>

  <strong>Funding Structure:</strong><br>
  – Total accepted budget: €195,530.02<br>
  – CVC share: €84,446.05 (43.19%)<br>
  – Nubilum SL share: €111,083.98 (56.81%)<br><br>

  <strong>Team:</strong><br>
  – Dr. Lluís Gómez (CVC Lead)  
  – Pep Casals Pug (Nubilum Lead)
  – Marc Folia Campos (Nubilum)  
  – Francesc Net Barnes (CVC research staff)<br><br>

  <strong>Keywords:</strong> Cultural Heritage, AI for Archives, Multimodal Indexing, Recommendation Systems, Computer Vision, NLP<br><br>

  <strong>More Info:</strong>  
  <a href="https://youtu.be/DyKLoqyrPsA?list=PLPMj2J--V29dgpEtCBhvmqWBDY51gkvLv&t=231" target="_blank">Video</a> • 
  <a href="https://coeli.cat/" target="_blank">coeli.cat</a> • 
  <a href="https://cvc.uab.cat" target="_blank">cvc.uab.cat</a>
</details>

<br>

## Past Projects

- **ReadQA** – Reading systems for Visual Question Answering  
  _Funded by:_ Ministerio de Ciencia e Innovación (PID2020-116298GB-I00) • **€89,419**  
  _Period:_ Jan 2021 – Dec 2023 • _PIs:_ Lluis Gomez & Dimosthenis Karatzas  
  Aimed to improve scene-text-based VQA systems using advanced multimodal models.

- **BeARS** – Beyond Automatic Reading Systems  
  _Funded by:_ AGAUR (Catalan University and Research Agency) • **€97,000**  
  _Period:_ 2020–2021 • _PIs:_ M. Russiñol & Lluis Gomez  
  Focused on broadening the capabilities of reading systems beyond OCR.

- **DeepPhotoArchive**  
  _Funded by:_ TECNIOspring PLUS / H2020 MSCA / ACCIÓ • **€113,339**  
  _Period:_ 2018–2020 • _PI:_ Lluis Gomez  
  Applied deep learning to build semantic search engines for photo archives.

- **READS** – Reading the Scene  
  _Funded by:_ Ministerio de Economía, Industria y Competitividad (TIN2017-89779P) • **€81,554**  
  _Period:_ 2018–2020 • _PIs:_ D. Karatzas & E. Valveny  
  Core research on text-in-scene interpretation and representation.

- **Semantic Search in Digital Newspaper Libraries**  
  _Funded by:_ Fundación BBVA • **€74,526**  
  _Period:_ 2018–2019 • _PI:_ M. Russiñol • _Role:_ Core Researcher  
  Developed multimodal search tools for historical digital newspapers.

- **RAW** – Reading in the Wild  
  _Funded by:_ Ministerio de Economía y Competitividad (TIN2014-52072P) • **€109,021**  
  _Period:_ 2015–2017 • _PI:_ D. Karatzas • _Role:_ Core Researcher  
  Addressed robust scene text understanding in unconstrained environments.

- **Text and the City** – Human-Centred Scene Text Understanding  
  _Funded by:_ Ministerio de Ciencia e Innovación (TIN2011-24631) • **€78,045**  
  _Period:_ 2012–2014 • _PI:_ D. Karatzas • _Role:_ Core Researcher  
  Explored user-centric models for text interpretation in urban imagery.

- **Knowledge Extraction from Document Images with Heterogeneous Contents**  
  _Funded by:_ Ministerio de Ciencia e Innovación (TIN2009-14633-C03-03) • **€195,000**  
  _Period:_ Jan 2010 – Aug 2013 • _PI:_ J. Lladós • _Role:_ Core Researcher  
  Investigated document image understanding for structured and unstructured content.

- **HuPerText** – Human Perception Inspired Text Technologies  
  _Funded by:_ Ministerio de Ciencia e Innovación (TIN2008-04998) • **€49,610**  
  _Period:_ 2009–2011 • _PI:_ D. Karatzas • _Role:_ Core Researcher  
  Focused on perceptually motivated scene text modeling and reading.

