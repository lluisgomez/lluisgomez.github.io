@inproceedings{gomez2025sigir,
  author    = {Lluis Gomez},
  title     = {Measuring Text-Image Retrieval Fairness with Synthetic Data},
  booktitle = {Proceedings of the 48th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  year      = {2025},
  note      = {To appear}
}
,
@inproceedings{gomez2025uai,
  author    = {Lluis Gomez},
  title     = {Over the Top-1: Uncertainty-Aware Cross-Modal Retrieval with CLIP},
  booktitle = {Proceedings of the 41st Conference on Uncertainty in Artificial Intelligence (UAI)},
  year      = {2025},
  note      = {To appear}
}
,
@incollection{ruiz2025women,
  author    = {Sonia Ruiz and Lluis Gomez},
  title     = {Charting Pathways: An Intersectional Impact Assessment for Vision and Language Foundation Models},
  booktitle = {Women, Technology, and Power - Unmasking (and dealing with) digital disparities in the times of the platforms},
  editor    = {Miren Gutiérrez and Antonia Moreno Cano},
  publisher = {Editorial Tirant Lo Blanch},
  year      = {2025},
  isbn      = {9788410811102},
  note      = {1st Edition, 264 pages, Plural Collection},
  language  = {English},
  type      = {Book Chapter}
}
,
@inproceedings{elaichouni2025ibpria,
  author    = {Mustapha El Aichouni and Lluis Gomez and Lei Kang},
  title     = {Mitigating Distribution Bias in Multimodal Datasets via Clustering-Based Curation},
  booktitle = {Iberian Conference on Pattern Recognition and Image Analysis (IbPRIA)},
  year      = {2025},
  note      = {To appear}
}
,
 @article{Net_2025, title={EUFCC-340K: A faceted hierarchical dataset for metadata annotation in GLAM collections}, ISSN={1573-7721}, url={http://dx.doi.org/10.1007/s11042-024-20561-9}, DOI={10.1007/s11042-024-20561-9}, journal={Multimedia Tools and Applications}, publisher={Springer Science and Business Media LLC}, author={Net, Francesc and Folia, Marc and Casals, Pep and Bagdanov, Andrew D. and Gómez, Lluis}, year={2025}, month=jan }
,
 @inbook{Net_2025, title={EUFCC-CIR: A Composed Image Retrieval Dataset for GLAM Collections}, ISBN={9783031915727}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-91572-7_12}, DOI={10.1007/978-3-031-91572-7_12}, booktitle={Computer Vision – ECCV 2024 Workshops}, publisher={Springer Nature Switzerland}, author={Net, Francesc and Gomez, Lluis}, year={2025}, pages={196–211} }
,
 @article{kang2025preserving,
  title={Preserving Privacy Without Compromising Accuracy: Machine Unlearning for Handwritten Text Recognition},
  author={Kang, Lei and Fu, Xuanshuo and Gomez, Lluis and Fornés, Alicia and Valveny, Ernest and Karatzas, Dimosthenis},
  journal={arXiv preprint arXiv:2504.08616},
  year={2025}
}
,
 @inbook{Net_2024, title={A Transformer-Based Object-Centric Approach for Date Estimation of Historical Photographs}, ISBN={9783031560637}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-56063-7_9}, DOI={10.1007/978-3-031-56063-7_9}, booktitle={Advances in Information Retrieval}, publisher={Springer Nature Switzerland}, author={Net, Francesc and Hernández, Núria and Molina, Adriá and Gómez, Lluis}, year={2024}, pages={137–150} }
,
 @inbook{Kang_2024, title={Machine Unlearning for Document Classification}, ISBN={9783031705465}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-70546-5_6}, DOI={10.1007/978-3-031-70546-5_6}, booktitle={Document Analysis and Recognition - ICDAR 2024}, publisher={Springer Nature Switzerland}, author={Kang, Lei and Souibgui, Mohamed Ali and Yang, Fei and Gomez, Lluis and Valveny, Ernest and Karatzas, Dimosthenis}, year={2024}, pages={90–102} }
,
@incollection{kang2024grif,
  title={GRIF-DM: Generation of Rich Impression Fonts using Diffusion Models},
  author={Kang, Lei and Yang, Fei and Wang, Kai and Souibgui, Mohamed Ali and Gomez, Lluis and Fornés, Alicia and Valveny, Ernest and Karatzas, Dimosthenis},
  booktitle={European Conference on Artificial Intelligence 2024},
  pages={226--233},
  year={2024},
  publisher={IOS Press}
}
,
@article{Nguyen_2023, title={Show, Interpret and Tell: Entity-Aware Contextualised Image Captioning in Wikipedia}, volume={37}, ISSN={2159-5399}, url={http://dx.doi.org/10.1609/aaai.v37i2.25285}, DOI={10.1609/aaai.v37i2.25285}, number={2}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, publisher={Association for the Advancement of Artificial Intelligence (AAAI)}, author={Nguyen, Khanh and Biten, Ali Furkan and Mafla, Andres and Gomez, Lluis and Karatzas, Dimosthenis}, year={2023}, month=jun, pages={1940–1948} }
,
 @article{Souibgui_2023, title={Text-DIAE: A Self-Supervised Degradation Invariant Autoencoder for Text Recognition and Document Enhancement}, volume={37}, ISSN={2159-5399}, url={http://dx.doi.org/10.1609/aaai.v37i2.25328}, DOI={10.1609/aaai.v37i2.25328}, number={2}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, publisher={Association for the Advancement of Artificial Intelligence (AAAI)}, author={Souibgui, Mohamed Ali and Biswas, Sanket and Mafla, Andres and Biten, Ali Furkan and Fornés, Alicia and Kessentini, Yousri and Lladós, Josep and Gomez, Lluis and Karatzas, Dimosthenis}, year={2023}, month=jun, pages={2330–2338} }
,
 @inbook{Vivoli_2023, title={MUST-VQA: MUltilingual Scene-Text VQA}, ISBN={9783031250699}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-25069-9_23}, DOI={10.1007/978-3-031-25069-9_23}, booktitle={Computer Vision – ECCV 2022 Workshops}, publisher={Springer Nature Switzerland}, author={Vivoli, Emanuele and Biten, Ali Furkan and Mafla, Andres and Karatzas, Dimosthenis and Gomez, Lluis}, year={2023}, pages={345–358} }
,
@inbook{36566f537bd94150b8afdc26f309026e,
  title     = "OCR-IDL: OCR Annotations for Industry Document Library Dataset",
  abstract  = "Pretraining has proven successful in Document Intelligence tasks where deluge of documents are used to pretrain the models only later to be finetuned on downstream tasks. One of the problems of the pretraining approaches is the inconsistent usage of pretraining data with different OCR engines leading to incomparable results between models. In other words, it is not obvious whether the performance gain is coming from diverse usage of amount of data and distinct OCR engines or from the proposed models. To remedy the problem, we make public the OCR annotations for IDL documents using commercial OCR engine given their superior performance over open source OCR models. It is our hope that OCR-IDL can be a starting point for future works on Document Intelligence. All of our data and its collection process with the annotations can be found in https://github.com/furkanbiten/idl_data.",
  author    = "Biten, {Ali Furkan} and Rubèn Tito and Lluis Gomez and Ernest Valveny and Dimosthenis Karatzas",
  note      = "Funding Information: Acknowledgments. This work has been supported by projects PDC2021-121512-I00, PLEC2021-00785, PID2020-116298GB-I00, ACE034/21/000084, the CERCA Programme / Generalitat de Catalunya, AGAUR project 2019PROD00090 (BeARS), the Ramon y Cajal RYC2020-030777-I / AEI / 10.13039/501100011033 and PhD scholarship from UAB (B18P0073). Publisher Copyright: {\textcopyright} 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.; 17th European Conference on Computer Vision, ECCV 2022 ; Conference date: 23-10-2022 Through 27-10-2022",
  year      = "2023",
  doi       = "10.1007/978-3-031-25069-9_16",
  language  = "English",
  isbn      = "9783031250682",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer Science and Business Media Deutschland GmbH",
  pages     = "241--252",
  editor    = "Leonid Karlinsky and Tomer Michaeli and Ko Nishino",
  booktitle = "Computer Vision – ECCV 2022 Workshops, Proceedings",
},
 @inbook{Net_2023, title={Transductive Learning for Near-Duplicate Image Detection in Scanned Photo Collections}, ISBN={9783031417344}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-031-41734-4_1}, DOI={10.1007/978-3-031-41734-4_1}, booktitle={Document Analysis and Recognition - ICDAR 2023}, publisher={Springer Nature Switzerland}, author={Net, Francesc and Folia, Marc and Casals, Pep and Gómez, Lluis}, year={2023}, pages={3–17} }
,
@book{772fd52cab1b40f98d7d0d2204a7a63e,
  title     = "A Generic Image Retrieval Method for Date Estimation of Historical Document Collections",
  abstract  = "Date estimation of historical document images is a challenging problem, with several contributions in the literature that lack of the ability to generalize from one dataset to others. This paper presents a robust date estimation system based in a retrieval approach that generalizes well in front of heterogeneous collections. We use a ranking loss function named smooth-nDCG to train a Convolutional Neural Network that learns an ordination of documents for each problem. One of the main usages of the presented approach is as a tool for historical contextual retrieval. It means that scholars could perform comparative analysis of historical images from big datasets in terms of the period where they were produced. We provide experimental evaluation on different types of documents from real datasets of manuscript and newspaper images.",
  keywords  = "Date estimation, Document retrieval, Image retrieval, Ranking loss, Smooth-nDCG",
  author    = "Adri{\`a} Molina and Lluis Gomez and {Ramos Terrades}, Oriol and Josep Lladós",
  note      = "Publisher Copyright: {\textcopyright} 2022, Springer Nature Switzerland AG.; 15th IAPR International Workshop on Document Analysis Systems, DAS 2022 ; Conference date: 22-05-2022 Through 25-05-2022",
  year      = "2022",
  doi       = "10.1007/978-3-031-06555-2_39",
  language  = "English",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
},
@inbook{88b3bfcb126a44699cdc7a35201d5863,
  title     = "A Multilingual Approach to Scene Text Visual Question Answering",
  abstract  = "Scene Text Visual Question Answering (ST-VQA) has recently emerged as a hot research topic in Computer Vision. Current ST-VQA models have a big potential for many types of applications but lack the ability to perform well on more than one language at a time due to the lack of multilingual data, as well as the use of monolingual word embeddings for training. In this work, we explore the possibility to obtain bilingual and multilingual VQA models. In that regard, we use an already established VQA model that uses monolingual word embeddings as part of its pipeline and substitute them by FastText and BPEmb multilingual word embeddings that have been aligned to English. Our experiments demonstrate that it is possible to obtain bilingual and multilingual VQA models with a minimal loss in performance in languages not used during training, as well as a multilingual model trained in multiple languages that match the performance of the respective monolingual baselines.",
  keywords  = "Deep learning, Multilingual word embeddings, Scene text, Vision and language, Visual question answering",
  author    = "{Brugués i Pujolràs}, Josep and Gómez i Bigordà, Lluis and Dimosthenis Karatzas",
  note      = "Funding Information: Acknowledgment. This work has been supported by: Grant PDC2021-121512-I00 funded by MCIN /AEI/10.13039/501100011033 and the European Union NextGen-erationEU/PRTR; Project PID2020-116298GB-I00 funded by MCIN/ AEI /10.13039/501100011033; Grant PLEC2021-007850 funded by MCIN/AEI/10.13039/501100011033 and the European Union NextGenerationEU/PRTR. Publisher Copyright: {\textcopyright} 2022, Springer Nature Switzerland AG.; 15th IAPR International Workshop on Document Analysis Systems, DAS 2022 ; Conference date: 22-05-2022 Through 25-05-2022",
  year      = "2022",
  doi       = "10.1007/978-3-031-06555-2_5",
  language  = "English",
  isbn      = "9783031065545",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer Science and Business Media Deutschland GmbH",
  pages     = "65--79",
  editor    = "Seiichi Uchida and Elisa Barney and Véronique Eglin",
  booktitle = "Document Analysis Systems - 15th IAPR International Workshop, DAS 2022, Proceedings",
},
@inbook{423baa78c7d04fe587d84fa951a87c80,
  title     = "Is An Image Worth Five Sentences? A New Look into Semantics for Image-Text Matching",
  abstract  = "The task of image-text matching aims to map representations from different modalities into a common joint visual-textual embedding. However, the most widely used datasets for this task, MSCOCO and Flickr30K, are actually image captioning datasets that offer a very limited set of relation-ships between images and sentences in their ground-truth annotations. This limited ground truth information forces us to use evaluation metrics based on binary relevance: given a sentence query we consider only one image as relevant. However, many other relevant images or captions may be present in the dataset. In this work, we propose two metrics that evaluate the degree of semantic relevance of retrieved items, independently of their annotated binary relevance. Additionally, we incorporate a novel strategy that uses an image captioning metric, CIDEr, to define a Semantic Adaptive Margin (SAM) to be optimized in a standard triplet loss. By incorporating our formulation to existing models, a large improvement is obtained in scenarios where available training data is limited. We also demonstrate that the performance on the annotated image-caption pairs is maintained while improving on other non-annotated relevant items when employing the full training set. The code for our new metric can be found at github.com/furkanbiten/ncs_metric and the model implementation at github.com/andrespmd/semantic_adaptive_margin.",
  keywords  = "Vision and Languages",
  author    = "Biten, {Ali Furkan} and Andres Mafla and Lluis Gomez and Dimosthenis Karatzas",
  note      = "Funding Information: This work has been supported by projects PID2020-116298GB-I00, the CERCA Programme / Generalitat de Catalunya, AGAUR project 2019PROD00090 (BeARS) and PhD scholarships from AGAUR (2019-FIB01233) and UAB (B18P0073). Publisher Copyright: {\textcopyright} 2022 IEEE.; 22nd IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022 ; Conference date: 04-01-2022 Through 08-01-2022",
  year      = "2022",
  doi       = "10.1109/WACV51458.2022.00254",
  language  = "English",
  series    = "Proceedings - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  pages     = "2483--2492",
  booktitle = "Proceedings - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022",
  address   = "United States",
},
@book{bcb05edd9e284719aa94a2e08c64cb5f,
  title     = "Let there be a clock on the beach: Reducing Object Hallucination in Image Captioning",
  abstract  = "Explaining an image with missing or non-existent objects is known as object bias (hallucination) in image captioning. This behaviour is quite common in the state-of-the-art captioning models which is not desirable by humans. To decrease the object hallucination in captioning, we propose three simple yet efficient training augmentation method for sentences which requires no new training data or increase in the model size. By extensive analysis, we show that the proposed methods can significantly diminish our models' object bias on hallucination metrics. Moreover, we experimentally demonstrate that our methods decrease the dependency on the visual features. All of our code, configuration files and model weights are available online1.",
  keywords  = "Vision and Languages",
  author    = "Biten, {Ali Furkan} and Lluis Gomez and Dimosthenis Karatzas",
  note      = "Funding Information: This work has been supported by projects PID2020-116298GB-I00, the CERCA Programme / Generalitat de Catalunya, AGAUR project 2019PROD00090 (BeARS) and PhD scholarship from UAB (B18P0073). Publisher Copyright: {\textcopyright} 2022 IEEE.; 22nd IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022 ; Conference date: 04-01-2022 Through 08-01-2022",
  year      = "2022",
  doi       = "10.1109/WACV51458.2022.00253",
  language  = "English",
  series    = "Proceedings - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  address   = "United States",
},
@inbook{22a8ad851af843398d69be01b91613c1,
  title     = "One-shot Compositional Data Generation for Low Resource Handwritten Text Recognition",
  abstract  = "Low resource Handwritten Text Recognition (HTR) is a hard problem due to the scarce annotated data and the very limited linguistic information (dictionaries and language models). For example, in the case of historical ciphered manuscripts, which are usually written with invented alphabets to hide the message contents. Thus, in this paper we address this problem through a data generation technique based on Bayesian Program Learning (BPL). Contrary to traditional generation approaches, which require a huge amount of annotated images, our method is able to generate human-like handwriting using only one sample of each symbol in the alphabet. After generating symbols, we create synthetic lines to train state-of-the-art HTR architectures in a segmentation free fashion. Quantitative and qualitative analyses were carried out and confirm the effectiveness of the proposed method.",
  keywords  = "Document Analysis",
  author    = "Souibgui, {Mohamed Ali} and Biten, {Ali Furkan} and Sounak Dey and Alicia Fornes and Yousri Kessentini and Lluis Gomez and Dimosthenis Karatzas and Josep Llados",
  note      = "Funding Information: This work has been partially supported by the Swedish Research Council (grant 2018-06074, DECRYPT), the Spanish project RTI2018-095645-B-C21, the CERCA Program / Generalitat de Catalunya, the project PID2020-116298GB-I00, AGAUR project 2019PROD00090 (BeARS) and PhD scholarships from UAB (B18P0073). Publisher Copyright: {\textcopyright} 2022 IEEE.; 22nd IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022 ; Conference date: 04-01-2022 Through 08-01-2022",
  year      = "2022",
  doi       = "10.1109/WACV51458.2022.00262",
  language  = "English",
  series    = "Proceedings - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  pages     = "2563--2571",
  booktitle = "Proceedings - 2022 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2022",
  address   = "United States",
},
@article{9b110ef07dab434492e3b0341c696f18,
  title     = "Multimodal grid features and cell pointers for scene text visual question answering",
  abstract  = "This paper presents a new model for the task of scene text visual question answering. In this task questions about a given image can only be answered by reading and understanding scene text. Current state of the art models for this task make use of a dual attention mechanism in which one attention module attends to visual features while the other attends to textual features. A possible issue with this is that it makes difficult for the model to reason jointly about both modalities. To fix this problem we propose a new model that is based on an single attention mechanism that attends to multi-modal features conditioned to the question. The output weights of this attention module over a grid of multi-modal spatial features are interpreted as the probability that a certain spatial location of the image contains the answer text to the given question. Our experiments demonstrate competitive performance in two standard datasets with a model that is ×5 faster than previous methods at inference time. Furthermore, we also provide a novel analysis of the ST-VQA dataset based on a human performance study. Supplementary material, code, and data is made available through this link.",
  keywords  = "41A05, 41A10, 65D05, 65D17, Deep learning, MSC, Multi-modal learning, Scene text, Visual question answering",
  author    = "Lluís Gómez and Biten, {Ali Furkan} and Tito, {Rubèn Pérez} and Andrés Mafla and Mar{\c c}al Rusi{\~n}ol and Ernest Valveny and Dimosthenis Karatzas",
  note      = "Funding Information: This work has been partially supported by the Spanish research project TIN2014-52072-P and the CERCA Programme / Generalitat de Catalunya. We gratefully acknowledge the support of the NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research. Publisher Copyright: {\textcopyright} 2021",
  year      = "2021",
  month     = oct,
  doi       = "10.1016/J.PATREC.2021.06.026",
  language  = "English",
  volume    = "150",
  pages     = "242--249",
  journal   = "Pattern Recognition Letters",
  issn      = "0167-8655",
  publisher = "Elsevier",
},
@article{e1d85b32106145ed80c5eb6e42e4e921,
  title     = "Asking questions on handwritten document collections",
  abstract  = "This work addresses the problem of Question Answering (QA) on handwritten document collections. Unlike typical QA and Visual Question Answering (VQA) formulations where the answer is a short text, we aim to locate a document snippet where the answer lies. The proposed approach works without recognizing the text in the documents. We argue that the recognition-free approach is suitable for handwritten documents and historical collections where robust text recognition is often difficult. At the same time, for human users, document image snippets containing answers act as a valid alternative to textual answers. The proposed approach uses an off-the-shelf deep embedding network which can project both textual words and word images into a common sub-space. This embedding bridges the textual and visual domains and helps us retrieve document snippets that potentially answer a question. We evaluate results of the proposed approach on two new datasets: (i) HW-SQuAD: a synthetic, handwritten document image counterpart of SQuAD1.0 dataset and (ii) BenthamQA: a smaller set of QA pairs defined on documents from the popular Bentham manuscripts collection. We also present a thorough analysis of the proposed recognition-free approach compared to a recognition-based approach which uses text recognized from the images using an OCR. Datasets presented in this work are available to download at docvqa.org.",
  keywords  = "Handwritten documents, Information retrieval, Question Answering",
  author    = "Minesh Mathew and Lluis Gomez and Dimosthenis Karatzas and Jawahar, {C. V.}",
  note      = "Publisher Copyright: {\textcopyright} 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",
  year      = "2021",
  month     = sep,
  doi       = "10.1007/s10032-021-00383-3",
  language  = "English",
  volume    = "24",
  pages     = "235--249",
  journal   = "International journal on document analysis and recognition",
  issn      = "1433-2833",
  publisher = "Springer-Verlag",
  number    = "3",
},
@article{Mafla_2021,
	doi = {10.1016/j.patcog.2020.107656},
	url = {https://doi.org/10.1016%2Fj.patcog.2020.107656},
	year = 2021,
	month = {feb},
	publisher = {Elsevier {BV}},
	volume = {110},
	pages = {107656},
	author = {Andrés Mafla and Rubèn Tito and Sounak Dey and Lluis Gomez and Marçal Rusiñol and Ernest Valveny and Dimosthenis Karatzas},
	title = {Real-time Lexicon-free Scene Text Retrieval},
	journal = {Pattern Recognition}
},
@book{e8b64b34968d44f58bce3c80a85fa2ff,
  title     = "StacMR: Scene-text aware cross-modal retrieval",
  abstract  = "Recent models for cross-modal retrieval have benefited from an increasingly rich understanding of visual scenes, afforded by scene graphs and object interactions to mention a few. This has resulted in an improved matching between the visual representation of an image and the textual representation of its caption. Yet, current visual representations overlook a key aspect: the text appearing in images, which may contain crucial information for retrieval. In this paper, we first propose a new dataset that allows exploration of cross-modal retrieval where images contain scene-text instances. Then, armed with this dataset, we describe several approaches which leverage scene text, including a better scene-text aware cross-modal retrieval method which uses specialized representations for text from the captions and text from the visual scene, and reconcile them in a common embedding space. Extensive experiments confirm that cross-modal retrieval approaches benefit from scene text and highlight interesting research questions worth exploring further. Dataset and code are available at europe.naverlabs.com/stacmr.",
  author    = "Andres Mafla and Rezende, {Rafael S.} and Lluis Gomez and Diane Larlus and Dimosthenis Karatzas",
  note      = "Publisher Copyright: {\textcopyright} 2021 IEEE.; 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021 ; Conference date: 05-01-2021 Through 09-01-2021",
  year      = "2021",
  month     = jan,
  doi       = "10.1109/WACV48630.2021.00227",
  language  = "English",
  series    = "Proceedings - 2021 IEEE Winter Conference on Applications of Computer Vision, WACV 2021",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  address   = "United States",
},
 @inbook{Molina_2021, title={Date Estimation in the Wild of Scanned Historical Photos: An Image Retrieval Approach}, ISBN={9783030863319}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-86331-9_20}, DOI={10.1007/978-3-030-86331-9_20}, booktitle={Document Analysis and Recognition – ICDAR 2021}, publisher={Springer International Publishing}, author={Molina, Adrià and Riba, Pau and Gomez, Lluis and Ramos-Terrades, Oriol and Lladós, Josep}, year={2021}, pages={306–320} }
,
@book{da9318e54cb04a73bb669570e00038e2,
  title     = "Learning to Rank Words: Optimizing Ranking Metrics for Word Spotting",
  abstract  = "In this paper, we explore and evaluate the use of ranking-based objective functions for learning simultaneously a word string and a word image encoder. We consider retrieval frameworks in which the user expects a retrieval list ranked according to a defined relevance score. In the context of a word spotting problem, the relevance score has been set according to the string edit distance from the query string. We experimentally demonstrate the competitive performance of the proposed model on query-by-string word spotting for both, handwritten and real scene word images. We also provide the results for query-by-example word spotting, although it is not the main focus of this work.",
  keywords  = "Ranking loss, Smooth-AP, Smooth-nDCG, Word spotting",
  author    = "Pau Riba and Adri{\`a} Molina and Lluis Gomez and Oriol Ramos-Terrades and Josep Lladós",
  note      = "Publisher Copyright: {\textcopyright} 2021, Springer Nature Switzerland AG.; 16th International Conference on Document Analysis and Recognition, ICDAR 2021 ; Conference date: 05-09-2021 Through 10-09-2021",
  year      = "2021",
  doi       = "10.1007/978-3-030-86331-9_25",
  language  = "English",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer Science and Business Media Deutschland GmbH",
},
@inbook{812d82a58e9043e88e35d02f82523eab,
  title     = "RoadText-1K: Text Detection Recognition Dataset for Driving Videos",
  abstract  = "Perceiving text is crucial to understand semantics of outdoor scenes and hence is a critical requirement to build intelligent systems for driver assistance and self-driving. Most of the existing datasets for text detection and recognition comprise still images and are mostly compiled keeping text in mind. This paper introduces a new RoadText-1K dataset for text in driving videos. The dataset is 20 times larger than the existing largest dataset for text in videos. Our dataset comprises 1000 video clips of driving without any bias towards text and with annotations for text bounding boxes and transcriptions in every frame. State of the art methods for text detection, recognition and tracking are evaluated on the new dataset and the results signify the challenges in unconstrained driving videos compared to existing datasets. This suggests that RoadText-1K is suited for research and development of reading systems, robust enough to be incorporated into more complex downstream tasks like driver assistance and self-driving. The dataset can be found at http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtext-1k.",
  author    = "Sangeeth Reddy and Minesh Mathew and Lluis Gomez and Marcal Rusinol and Dimosthenis Karatzas and Jawahar, {C. V.}",
  note      = "Publisher Copyright: {\textcopyright} 2020 IEEE.; 2020 IEEE International Conference on Robotics and Automation, ICRA 2020 ; Conference date: 31-05-2020 Through 31-08-2020",
  year      = "2020",
  month     = may,
  doi       = "10.1109/ICRA40945.2020.9196577",
  language  = "English",
  series    = "Proceedings - IEEE International Conference on Robotics and Automation",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  pages     = "11074--11080",
  booktitle = "2020 IEEE International Conference on Robotics and Automation, ICRA 2020",
  address   = "United States",
},
@inbook{0fcbd54dad57485b8b0343cd3ba52bbd,
  title     = "Exploring hate speech detection in multimodal publications",
  abstract  = "In this work we target the problem of hate speech detection in multimodal publications formed by a text and an image. We gather and annotate a large scale dataset from Twitter, MMHS150K, and propose different models that jointly analyze textual and visual information for hate speech detection, comparing them with unimodal detection. We provide quantitative and qualitative results and analyze the challenges of the proposed task. We find that, even though images are useful for the hate speech detection task, current multimodal models cannot outperform models analyzing only text. We discuss why and open the field and the dataset for further research.",
  author    = "Raul Gomez and Jaume Gibert and Lluis Gomez and Dimosthenis Karatzas",
  note      = "Publisher Copyright: {\textcopyright} 2020 IEEE.; 2020 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2020 ; Conference date: 01-03-2020 Through 05-03-2020",
  year      = "2020",
  month     = mar,
  doi       = "10.1109/WACV45572.2020.9093414",
  language  = "English",
  series    = "Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  pages     = "1459--1467",
  booktitle = "Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020",
  address   = "United States",
},
@inbook{c478b112cff8433f8642a6f39e712f06,
  title     = "Fine-grained image classification and retrieval by combining visual and locally pooled textual features",
  abstract  = "Text contained in an image carries high-level semantics that can be exploited to achieve richer image understanding. In particular, the mere presence of text provides strong guiding content that should be employed to tackle a diversity of computer vision tasks such as image retrieval, fine-grained classification, and visual question answering. In this paper, we address the problem of fine-grained classification and image retrieval by leveraging textual information along with visual cues to comprehend the existing intrinsic relation between the two modalities. The novelty of the proposed model consists of the usage of a PHOC descriptor to construct a bag of textual words along with a Fisher Vector Encoding that captures the morphology of text. This approach provides a stronger multimodal representation for this task and as our experiments demonstrate, it achieves state-of-the-art results on two different tasks, fine-grained classification and image retrieval. The code of this model will be publicly available at1.",
  author    = "Andres Mafla and Sounak Dey and Biten, {Ali Furkan} and Lluis Gomez and Dimosthenis Karatzas",
  note      = "Publisher Copyright: {\textcopyright} 2020 IEEE.; 2020 IEEE/CVF Winter Conference on Applications of Computer Vision, WACV 2020 ; Conference date: 01-03-2020 Through 05-03-2020",
  year      = "2020",
  month     = mar,
  doi       = "10.1109/WACV45572.2020.9093373",
  language  = "English",
  series    = "Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  pages     = "2939--2948",
  booktitle = "Proceedings - 2020 IEEE Winter Conference on Applications of Computer Vision, WACV 2020",
  address   = "United States",
},
@inbook{6a6b1e88469045328504c26ce61a8e92,
  title     = "Location Sensitive Image Retrieval and Tagging",
  abstract  = "People from different parts of the globe describe objects and concepts in distinct manners. Visual appearance can thus vary across different geographic locations, which makes location a relevant contextual information when analysing visual data. In this work, we address the task of image retrieval related to a given tag conditioned on a certain location on Earth. We present LocSens, a model that learns to rank triplets of images, tags and coordinates by plausibility, and two training strategies to balance the location influence in the final ranking. LocSens learns to fuse textual and location information of multimodal queries to retrieve related images at different levels of location granularity, and successfully utilizes location information to improve image tagging.",
  author    = "Raul Gomez and Jaume Gibert and Lluis Gomez and Dimosthenis Karatzas",
  note      = "Funding Information: Work supported by project TIN2017-89779-P, the CERCA Pro-gramme/Generalitat de Catalunya and the PhD scholarship AGAUR 2016-DI-84. Funding Information: Acknowledgement. Work supported by project TIN2017-89779-P, the CERCA Programme/Generalitat de Catalunya and the PhD scholarship AGAUR 2016-DI-84. Publisher Copyright: {\textcopyright} 2020, Springer Nature Switzerland AG.; 16th European Conference on Computer Vision, ECCV 2020 ; Conference date: 23-08-2020 Through 28-08-2020",
  year      = "2020",
  doi       = "10.1007/978-3-030-58517-4_38",
  language  = "English",
  isbn      = "9783030585167",
  series    = "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)",
  publisher = "Springer Science and Business Media Deutschland GmbH",
  pages     = "649--665",
  editor    = "Andrea Vedaldi and Horst Bischof and Thomas Brox and Jan-Michael Frahm",
  booktitle = "Computer Vision – ECCV 2020 - 16th European Conference, Proceedings",
},
@book{bdaa5fd7a9e34b87bbaa0ef9b90537b6,
  title     = "Text recognition - Real world data and where to find them",
  abstract  = "We present a method for exploiting weakly annotated images to improve text extraction pipelines. The approach uses an arbitrary end-to-end text recognition system to obtain text region proposals and their, possibly erroneous, transcriptions. The method includes matching of imprecise transcriptions to weak annotations and an edit distance guided neighbourhood search. It produces nearly error-free, localised instances of scene text, which we treat as “pseudo ground truth” (PGT). The method is applied to two weakly-annotated datasets. Training with the extracted PGT consistently improves the accuracy of a state of the art recognition model, by 3.7% on average, across different benchmark datasets (image domains) and 24.5% on one of the weakly annotated datasets.",
  author    = "Klara Janouskova and Jiri Matas and Lluis Gomez and Dimosthenis Karatzas",
  note      = "Funding Information: 1Acknowledgements. The authors were supported by Czech Technical University student grant SGS20/171/OHK3/3T/13, the MEYS VVV project CZ.02.1.01/0.0/0.0/16 019/0000765 Research Center for Informatics, the Spanish Research project TIN2017-89779-P and the CERCA Programme / Generalitat de Catalunya Publisher Copyright: {\textcopyright} 2020 IEEE; 25th International Conference on Pattern Recognition, ICPR 2020 ; Conference date: 10-01-2021 Through 15-01-2021",
  year      = "2020",
  doi       = "10.1109/ICPR48806.2021.9412868",
  language  = "English",
  series    = "Proceedings - International Conference on Pattern Recognition",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  address   = "United States",
},
@inbook{60df40bf577646d8aa779dc12132bf5d,
  title     = "Scene text visual question answering",
  abstract  = "Current visual question answering datasets do not consider the rich semantic information conveyed by text within an image. In this work, we present a new dataset, ST-VQA, that aims to highlight the importance of exploiting high-level semantic information present in images as textual cues in the Visual Question Answering process. We use this dataset to define a series of tasks of increasing difficulty for which reading the scene text in the context provided by the visual information is necessary to reason and generate an appropriate answer. We propose a new evaluation metric for these tasks to account both for reasoning errors as well as shortcomings of the text recognition module. In addition we put forward a series of baseline methods, which provide further insight to the newly released dataset, and set the scene for further research.",
  author    = "Biten, {Ali Furkan} and Ruben Tito and Andres Mafla and Lluis Gomez and Marcal Rusinol and Jawahar, {C. V.} and Ernest Valveny and DImosthenis Karatzas",
  note      = "Funding Information: This work has been supported by projects TIN2017-89779-P, Marie-Curie (712949 TECNIOspring PLUS), aBSINTHE (Fundacion BBVA 2017), the CERCA Programme / Generalitat de Catalunya, a European Social Fund grant (CCI: 2014ES05SFOP007), NVIDIA Corporation and PhD scholarships from AGAUR (2019-FIB01233) and the UAB. Publisher Copyright: {\textcopyright} 2019 IEEE.; 17th IEEE/CVF International Conference on Computer Vision, ICCV 2019 ; Conference date: 27-10-2019 Through 02-11-2019",
  year      = "2019",
  month     = oct,
  doi       = "10.1109/ICCV.2019.00439",
  language  = "English",
  series    = "Proceedings of the IEEE International Conference on Computer Vision",
  publisher = "Institute of Electrical and Electronics Engineers Inc.",
  pages     = "4290--4300",
  booktitle = "Proceedings - 2019 International Conference on Computer Vision, ICCV 2019",
  address   = "United States",
},
@inbook{167d63f0a92d497eb475e8d27365ab09,
  title     = "ICDAR 2019 competition on scene text visual question answering",
  abstract  = "This paper presents final results of ICDAR 2019 Scene Text Visual Question Answering competition (ST-VQA). ST-VQA introduces an important aspect that is not addressed by any Visual Question Answering system up to date, namely the incorporation of scene text to answer questions asked about an image. The competition introduces a new dataset comprising 23,038 images annotated with 31,791 question/answer pairs where the answer is always grounded on text instances present in the image. The images are taken from 7 different public computer vision datasets, covering a wide range of scenarios. The competition was structured in three tasks of increasing difficulty, that require reading the text in a scene and understanding it in the context of the scene, to correctly answer a given question. A novel evaluation metric is presented, which elegantly assesses both key capabilities expected from an optimal model: text recognition and image understanding. A detailed analysis of results from different participants is showcased, which provides insight into the current capabilities of VQA systems that can read. We firmly believe the dataset proposed in this challenge will be an important milestone to consider towards a path of more robust and general models that can exploit scene text to achieve holistic image understanding.",
  keywords  = "Scene text, Scene understanding, Vision and language, Visual question answering",
  author    = "{Furkan Biten}, Ali and Ruben Tito and Andres Mafla and Lluis Gomez and Marcal Rusinol and Minesh Mathew and Jawahar, {C. V.} and Ernest Valveny and Dimosthenis Karatzas",
  note      = "Funding Information: This work has been supported by projects TIN2017-89779-P, Marie-Curie (712949 TECNIOspring PLUS), aBSINTHE (Fundacion BBVA 2017), the CERCA Programme / Generalitat de Catalunya, a European Social Fund grant (CCI: 2014ES05SFOP007), NVIDIA Corporation and PhD scholarships from AGAUR (2019-FIB01233) and the UAB. Publisher Copyright: {\textcopyright} 2019 IEEE.; 15th IAPR International Conference on Document Analysis and Recognition, ICDAR 2019 ; Conference date: 20-09-2019 Through 25-09-2019",
  year      = "2019",
  month     = sep,
  doi       = "10.1109/ICDAR.2019.00251",
  language  = "English",
  series    = "Proceedings of the International Conference on Document Analysis and Recognition, ICDAR",
  publisher = "IEEE Computer Society Press",
  pages     = "1563--1570",
  booktitle = "Proceedings - 15th IAPR International Conference on Document Analysis and Recognition, ICDAR 2019",
},
@inbook{b8cf78497d49465d86f625b3f2854f6a,
  title     = "Selective style transfer for text",
  abstract  = "This paper explores the possibilities of image style transfer applied to text maintaining the original transcriptions. Results on different text domains (scene text, machine printed text and handwritten text) and cross-modal results demonstrate that this is feasible, and open different research lines. Furthermore, two architectures for selective style transfer, which means transferring style to only desired image pixels, are proposed. Finally, scene text selective style transfer is evaluated as a data augmentation technique to expand scene text detection datasets, resulting in a boost of text detectors performance. Our implementation of the described models is publicly available.",
  keywords  = "Data augmentation, Scene text detection, Style transfer, Text style transfer",
  author    = "Raul Gomez and Biten, {Ali Furkan} and Lluis Gomez and Jaume Gibert and Dimosthenis Karatzas and Marcal Rusinol",
  note      = "Funding Information: This work was supported by projects TIN2017-89779-P, Marie-Curie (712949 TECNIOspring PLUS), aBSINTHE (Fundacion BBVA 2017), Doctorats Industrials (AGAUR), the CERCA Programme/Generalitat de Catalunya, NVIDIA Corporation and a UAB PhD scholarship. Publisher Copyright: {\textcopyright} 2019 IEEE.; 15th IAPR International Conference on Document Analysis and Recognition, ICDAR 2019 ; Conference date: 20-09-2019 Through 25-09-2019",
  year      = "2019",
  month     = sep,
  doi       = "10.1109/ICDAR.2019.00134",
  language  = "English",
  series    = "Proceedings of the International Conference on Document Analysis and Recognition, ICDAR",
  publisher = "IEEE Computer Society Press",
  pages     = "805--812",
  booktitle = "Proceedings - 15th IAPR International Conference on Document Analysis and Recognition, ICDAR 2019",
},
@inbook{75effd397a334456a64c85ed56351eff,
  title     = "Good news, everyone! context driven entity-aware captioning for news images",
  abstract  = "Current image captioning systems perform at a merely descriptive level, essentially enumerating the objects in the scene and their relations. Humans, on the contrary, interpret images by integrating several sources of prior knowledge of the world. In this work, we aim to take a step closer to producing captions that offer a plausible interpretation of the scene, by integrating such contextual information into the captioning pipeline. For this we focus on the captioning of images used to illustrate news articles. We propose a novel captioning method that is able to leverage contextual information provided by the text of news articles associated with an image. Our model is able to selectively draw information from the article guided by visual cues, and to dynamically extend the output dictionary to out-of-vocabulary named entities that appear in the context source. Furthermore we introduce {"}GoodNews{"}, the largest news image captioning dataset in the literature and demonstrate state-of-the-art results.",
  keywords  = "Document Analysis, Vision + Language",
  author    = "Biten, {Ali Furkan} and Lluis Gomez and Marcal Rusinol and DImosthenis Karatzas",
  note      = "Funding Information: This work has been supported by projects TIN2017-89779-P, Marie-Curie (712949 TECNIOspring PLUS), aBSINTHE (Fun-dación BBVA 2017), the CERCA Programme / Generalitat de Catalunya, NVIDIA Corporation and a UAB PhD scholarship. Publisher Copyright: {\textcopyright} 2019 IEEE.; 32nd IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2019 ; Conference date: 16-06-2019 Through 20-06-2019",
  year      = "2019",
  month     = jun,
  doi       = "10.1109/CVPR.2019.01275",
  language  = "English",
  series    = "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",
  publisher = "IEEE Computer Society Press",
  pages     = "12458--12467",
  booktitle = "Proceedings - 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2019",
},
@article{Gómez2019,title = {FAST: Facilitated and Accurate Scene Text Proposals through FCN Guided Pruning},journal = {Pattern Recognition Letters},year = {2019},volume = {119},pages = {112-120},author = {Bazazian, D. and Gómez, R. and Nicolaou, A. and Gómez, L. and Karatzas, D. and Bagdanov, A.D.}},
@InProceedings{Biten_2019_CVPR, author= {Biten, Ali Furkan and Gomez, Lluis and Rusinol, Marcal and Karatzas, Dimosthenis}, title= {Good News, Everyone! Context Driven Entity-Aware Captioning for News Images}, booktitle= {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, month= {June}, year= {2019}}

,
 @inbook{Gomez_2019, title={Learning from #Barcelona Instagram Data What Locals and Tourists Post About Its Neighbourhoods}, ISBN={9783030110246}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-11024-6_41}, DOI={10.1007/978-3-030-11024-6_41}, booktitle={Computer Vision – ECCV 2018 Workshops}, publisher={Springer International Publishing}, author={Gomez, Raul and Gomez, Lluis and Gibert, Jaume and Karatzas, Dimosthenis}, year={2019}, pages={530–544} }
,
 @inbook{Gomez_2019, title={Learning to Learn from Web Data Through Deep Semantic Embeddings}, ISBN={9783030110246}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-11024-6_40}, DOI={10.1007/978-3-030-11024-6_40}, booktitle={Computer Vision – ECCV 2018 Workshops}, publisher={Springer International Publishing}, author={Gomez, Raul and Gomez, Lluis and Gibert, Jaume and Karatzas, Dimosthenis}, year={2019}, pages={514–529} }
,
@InProceedings{Biten_2019_ICCV, author= {Biten, Ali Furkan and Tito, Ruben and Mafla, Andres and Gomez, Lluis and Rusinol, Marcal and Valveny, Ernest and Jawahar, C.V. and Karatzas, Dimosthenis}, title= {Scene Text Visual Question Answering}, booktitle= {The IEEE International Conference on Computer Vision (ICCV)}, month= {October}, year= {2019}}

,
 @inbook{Gomez_2019, title={Self-Supervised Learning from Web Data for Multimodal Retrieval}, ISBN={9780128173589}, url={http://dx.doi.org/10.1016/b978-0-12-817358-9.00015-9}, DOI={10.1016/b978-0-12-817358-9.00015-9}, booktitle={Multimodal Scene Understanding}, publisher={Elsevier}, author={Gomez, Raul and Gomez, Lluis and Gibert, Jaume and Karatzas, Dimosthenis}, year={2019}, pages={279–306} }
,
@inproceedings{Patel_2019,doi = {10.1145/3323873.3325035},url = {https://doi.org/10.1145%2F3323873.3325035},year = 2019,publisher = {{ACM} Press},author = {Yash Patel and Lluis Gomez and Mar{\c{c}}al Rusi{\~{n}}ol and Dimosthenis Karatzas and C.V. Jawahar},title = {Self-Supervised Visual Representations for Cross-Modal Retrieval},booktitle = {Proceedings of the 2019 on International Conference on Multimedia Retrieval  - {ICMR} {\textquotesingle}19}},
@inproceedings{Gomez_2018,doi = {10.1109/das.2018.23},url = {https://doi.org/10.1109%2Fdas.2018.23},year = 2018,month = {apr},publisher = {{IEEE}},author = {Luis Gomez and Marcal Rusinol and Dimosthenis Karatzas},title = {Cutting Sayre{\textquotesingle}s Knot: Reading Scene Text without Segmentation. Application to Utility Meters},booktitle = {2018 13th {IAPR} International Workshop on Document Analysis Systems ({DAS})}},
@inproceedings{Karatzas_2018,doi = {10.1109/das.2018.22},url = {https://doi.org/10.1109%2Fdas.2018.22},year = 2018,month = {apr},publisher = {{IEEE}},author = {Dimosthenis Karatzas and Luis Gomez and Anguelos Nicolaou and Marcal Rusinol},title = {The Robust Reading Competition Annotation and Evaluation Platform},booktitle = {2018 13th {IAPR} International Workshop on Document Analysis Systems ({DAS})}},
 @inbook{G_mez_2018, title={Single Shot Scene Text Retrieval}, ISBN={9783030012649}, ISSN={1611-3349}, url={http://dx.doi.org/10.1007/978-3-030-01264-9_43}, DOI={10.1007/978-3-030-01264-9_43}, booktitle={Computer Vision – ECCV 2018}, publisher={Springer International Publishing}, author={Gómez, Lluís and Mafla, Andrés and Rusiñol, Marçal and Karatzas, Dimosthenis}, year={2018}, pages={728–744} }
,
@inproceedings{Gomez_2017,doi = {10.1109/icdar.2017.234},url = {https://doi.org/10.1109%2Ficdar.2017.234},year = 2017,month = {nov},publisher = {{IEEE}},author = {Raul Gomez and Baoguang Shi and Lluis Gomez and Lukas Numann and Andreas Veit and Jiri Matas and Serge Belongie and Dismosthenis Karatzas},title = {{ICDAR}2017 Robust Reading Challenge on {COCO}-Text},booktitle = {2017 14th {IAPR} International Conference on Document Analysis and Recognition ({ICDAR})}},
@inproceedings{Iwamura_2017,doi = {10.1109/icdar.2017.236},url = {https://doi.org/10.1109%2Ficdar.2017.236},year = 2017,month = {nov},publisher = {{IEEE}},author = {Masakazu Iwamura and Naoyuki Morimoto and Keishi Tainaka and Dena Bazazian and Lluis Gomez and Dimosthenis Karatzas},title = {{ICDAR}2017 Robust Reading Challenge on Omnidirectional Video},booktitle = {2017 14th {IAPR} International Conference on Document Analysis and Recognition ({ICDAR})}},
@inproceedings{Gomez_2017,doi = {10.1109/icdar.2017.88},url = {https://doi.org/10.1109%2Ficdar.2017.88},year = 2017,month = {nov},publisher = {{IEEE}},author = {Lluis Gomez and Marcal Rusinol and Dimosthenis Karatzas},title = {{LSDE}: Levenshtein Space Deep Embedding for Query-by-String Word Spotting},booktitle = {2017 14th {IAPR} International Conference on Document Analysis and Recognition ({ICDAR})}},
@article{G_mez_2017,doi = {10.1016/j.patcog.2017.04.027},url = {https://doi.org/10.1016%2Fj.patcog.2017.04.027},year = 2017,month = {oct},publisher = {Elsevier {BV}},volume = {70},pages = {60--74},author = {Lluis Gomez and Dimosthenis Karatzas},title = {{TextProposals}: A text-specific selective search algorithm for word spotting in the wild},journal = {Pattern Recognition}},
@inproceedings{Gomez_2017,doi = {10.1109/cvpr.2017.218},url = {https://doi.org/10.1109%2Fcvpr.2017.218},year = 2017,month = {jul},publisher = {{IEEE}},author = {Lluis Gomez and Yash Patel and Marcal Rusinol and Dimosthenis Karatzas and C. V. Jawahar},title = {Self-Supervised Learning of Visual Features through Embedding Images into Text Topic Spaces},booktitle = {2017 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR})}},
@article{Gomez2017,title = {Improving patch-based scene text script identification with ensembles of conjoined networks},journal = {Pattern Recognition},year = {2017},volume = {67},pages = {85-96},author = {Gomez, L. and Nicolaou, A. and Karatzas, D.}},
@book{Lluis_Gomez_42908630,
title={Exploiting similarity hierarchies for multi-script scene text understanding},
journal={UAB},
author={Lluis Gomez},
isbn={978\textendash84\textendash943427\textendash9\textendash0},
year={2016}
},
@inproceedings{Gomez_2016,doi = {10.1109/das.2016.64},url = {https://doi.org/10.1109%2Fdas.2016.64},year = 2016,month = {apr},publisher = {{IEEE}},author = {Lluis Gomez and Dimosthenis Karatzas},title = {A Fine-Grained Approach to Scene Text Script Identification},booktitle = {2016 12th {IAPR} Workshop on Document Analysis Systems ({DAS})}},
@inproceedings{Nicolaou_2016,doi = {10.1109/das.2016.63},url = {https://doi.org/10.1109%2Fdas.2016.63},year = 2016,month = {apr},publisher = {{IEEE}},author = {Anguelos Nicolaou and Andrew D. Bagdanov and Lluis Gomez and Dimosthenis Karatzas},title = {Visual Script and Language Identification},booktitle = {2016 12th {IAPR} Workshop on Document Analysis Systems ({DAS})}},
@article{Gomez2016,title = {A fast hierarchical method for multi-script and arbitrary oriented scene text extraction},journal = {International Journal on Document Analysis and Recognition},year = {2016},volume = {19},number = {4},pages = {335-349},author = {Gomez, L. and Karatzas, D.}},
@article{Gomez2016,title = {Dynamic lexicon generation for natural scene images},journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},year = {2016},volume = {9913 LNCS},pages = {395-410},author = {Patel, Y. and Gomez, L. and Rusi{\~n}ol, M. and Karatzas, D.}},
@inproceedings{Ghosh_2015,doi = {10.1109/icdar.2015.7333961},url = {https://doi.org/10.1109%2Ficdar.2015.7333961},year = 2015,month = {aug},publisher = {{IEEE}},author = {Suman K. Ghosh and Lluis Gomez and Dimosthenis Karatzas and Ernest Valveny},title = {Efficient indexing for Query By String text retrieval},booktitle = {2015 13th International Conference on Document Analysis and Recognition ({ICDAR})}},
@inproceedings{Gomez_2015,doi = {10.1109/icdar.2015.7333753},url = {https://doi.org/10.1109%2Ficdar.2015.7333753},year = 2015,month = {aug},publisher = {{IEEE}},author = {Lluis Gomez and Dimosthenis Karatzas},title = {Object proposals for text extraction in the wild},booktitle = {2015 13th International Conference on Document Analysis and Recognition ({ICDAR})}},
@article{Gomez2015,title = {ICDAR 2015 competition on Robust Reading},journal = {Proceedings of the International Conference on Document Analysis and Recognition, ICDAR},year = {2015},volume = {2015-November},pages = {1156-1160},author = {Karatzas, D. and Gomez-Bigorda, L. and Nicolaou, A. and Ghosh, S. and Bagdanov, A. and Iwamura, M. and Matas, J. and Neumann, L. and Chandrasekhar, V.R. and Lu, S. and Shafait, F. and Uchida, S. and Valveny, E.}},
@incollection{G_mez_2015,doi = {10.1007/978-3-319-16631-5_12},url = {https://doi.org/10.1007%2F978-3-319-16631-5_12},year = 2015,publisher = {Springer International Publishing},pages = {157--168},author = {Lluis Gomez and Dimosthenis Karatzas},title = {Scene Text Recognition: No Country for Old~Men?},booktitle = {Computer Vision - {ACCV} 2014 Workshops}},
@article{Gomez2014,title = {An on-line platform for ground truthing and performance evaluation of text extraction systems},journal = {Proceedings - 11th IAPR International Workshop on Document Analysis Systems, DAS 2014},year = {2014},pages = {242-246},author = {Karatzas, D. and Robles, S. and Gomez, L.}},
@article{Gomez2014,title = {MSER-based real-time text detection and tracking},journal = {Proceedings - International Conference on Pattern Recognition},year = {2014},pages = {3110-3115},author = {Gomez, L. and Karatzas, D.}},
@inproceedings{Karatzas_2013,doi = {10.1109/icdar.2013.221},url = {https://doi.org/10.1109%2Ficdar.2013.221},year = 2013,month = {aug},publisher = {{IEEE}},author = {Dimosthenis Karatzas and Faisal Shafait and Seiichi Uchida and Masakazu Iwamura and Lluis Gomez i Bigorda and Sergi Robles Mestre and Joan Mas and David Fernandez Mota and Jon Almazan Almazan and Lluis Pere de las Heras},title = {{ICDAR} 2013 Robust Reading Competition},booktitle = {2013 12th International Conference on Document Analysis and Recognition}},
@inproceedings{Gomez_2013,doi = {10.1109/icdar.2013.100},url = {https://doi.org/10.1109%2Ficdar.2013.100},year = 2013,month = {aug},publisher = {{IEEE}},author = {Lluis Gomez and Dimosthenis Karatzas},title = {Multi-script Text Extraction from Natural Scenes},booktitle = {2013 12th International Conference on Document Analysis and Recognition}},
@book{Lluis_Gomez_42908645,
title={Web semántica y gestión de archivos: una introducción},
journal={Gestión de la innovación y nuevas estrategias de investigación y difusión del fondo documental artístico},
author={Lluis Gomez},
isbn={978-84-9704-993-3},
year={2012}
}
